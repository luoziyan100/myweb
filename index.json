[{"content":"AI项目为何失败？数据决策者避坑指南 AI策略副总裁\n2025年1月5日\n阅读时间 15 分钟\n分享：\n人工智能（AI）已然颠覆全局，为各行各业许下效率、产能与创新的飞跃。然而，潜力虽好，许多公司在落地AI项目时却困难重重，结果往往未及预期。\n对于期望有效利用AI的组织而言，探究失败的根源至关重要。\n本文将探讨企业中AI项目失败的常见原因，并提供克服障碍的实用策略，以确保AI技术的成功整合与优化。\nAI项目为何折戟 AI项目之路，关卡重重，常常阻碍其进展与实效。其结果是，多数项目最终半途而废。那么，失败率究竟有多高？\n据《华尔街日报》近期一篇文章指出，AI项目的失败率高达50%。\n此外，IBM在其《2023年全球AI采用指数》报告中提到，导致AI项目失败的首要原因包括：AI专业知识有限（33%）、数据复杂性（25%）和伦理问题（23%）。\n让我们来看几个AI失败的实例，探究其为何没能达成积极的成果。\nAI失败实例 Meta的Galactica AI 2022年11月，Meta发布了名为Galactica的大型语言模型（LLM），并将其誉为一款能生成和总结科学内容的开创性工具。\n然而，Meta这款被寄予厚望的AI，其首次亮相很快就因缺陷毕露而演变成一场灾难。\nGalactica的设想是成为一个精密的AI，通过对复杂课题生成准确简洁的摘要，从而革新科学研究。Meta旨在为科研人员和教育者提供一个获取和传播科学知识的强大工具。\n尽管目标宏大，Galactica的表现却远未达到预期。它没有提供可靠、信息丰富的内容，反而输出了大量充斥着不准确、偏见和无稽之谈的文本。\n研究人员和用户很快发现，该AI生成的摘要常常具有误导性且缺乏可信度，这使得Galactica作为一个科学信息来源完全不可靠。\n人们担忧，这款AI可能会传播错误信息，损害科学言论的严肃性。顶尖的研究者和机构纷纷发声批评，警告世人依赖Galactica获取准确信息的危险。\n面对日益高涨的公众抵制和声誉受损的威胁，Meta别无选择，在Galactica发布数天后便叫停了该项目，等同于承认了这次雄心勃勃的AI探索以失败告终。\nGalactica的失败是一个警示，告诫我们AI技术固有的风险与局限，尤其是在科学研究等敏感领域。这场风波凸显了在开发和部署AI系统时，严格测试、验证和监督的重要性。\n加拿大航空的AI聊天机器人 2022年，加拿大航空的一名聊天机器人向一位顾客提供了关于丧亲旅行折扣的错误信息，导致该公司面临法律诉讼。\n尽管航空公司辩称，应为错误信息负责的是聊天机器人而非公司，但法庭最终裁定加拿大航空败诉。此案开创了美国法庭的先例，并凸显了AI生成内容可能带来的法律影响。\n此外，这项裁决也引发了关于在客户互动中使用AI技术的公司，其问责与责任归属的问题。随着AI在商业运营中扮演的角色日益重要，企业建立健全的机制来监控并确保AI生成内容的准确性，已变得至关重要。\n此案提醒我们，在部署AI时，透明、准确和问责至关重要，尤其是在客户信任与满意度至上的行业。\n纽约市的聊天机器人 纽约市一个旨在协助小企业的聊天机器人，给出了错误的法律建议。该机器人错误地暗示某些行为是合法的，例如解雇举报性骚扰的员工，或拒绝让员工保留其脏辫发型。\n此外，它还提供了关于废物和污水处理规定的不准确信息，并暗示餐厅可以提供被老鼠接触过的食物。\n为应对争议，该聊天机器人旁边的免责声明现已更新，以强调它不能提供法律建议。\n机器学习项目为何失败？ 随着企业越来越多地投资于AI驱动的解决方案以获取竞争优势，理解机器学习项目错综复杂的环境在当今技术生态中至关重要。\n尽管机器学习前景广阔，但现实是，大多数AI项目都会遇到意想不到的障碍，无法交付预期成果。\n在本节中，我们将深入探讨机器学习项目失败的多方面原因，揭示导致其衰落的技术、组织和战略因素之间复杂的相互作用。\n1. AI项目的风险与复杂性 尖端的机器学习模型和算法为工业应用提供了广阔天地。从生成复杂内容的简洁摘要，到精细分类客户反馈，再到通过GPT-4等创新技术组织非结构化数据，机器学习在不同领域的应用潜力前所未有。\n然而，随着机器学习模型训练所用的数据集日益多样——从传统的电子表格到复杂的音视频记录——风险管理成为一项艰巨的挑战。这种复杂性愈发凸显了制定稳健的风险管理策略以有效应对未知挑战的重要性。\n许多期望驾驭AI变革力量的组织，未能主动管理因数据多样且复杂而产生的风险，这正是AI项目失败的原因。实际上，商业AI项目的失败，往往与未能充分预见和减轻部署先进机器学习技术时固有的风险有关。\n2. 不合格的数据模型 Gartner指出，85%的AI项目之所以失败，主要原因是数据不准确和带有偏见。准确的数据收集是成功部署AI项目的两大障碍。\n数据的敏感性也可能是原因之一，尤其是在医疗等受到严格监管的领域。不准确的数据会损害AI模型的完整性，削弱其生成可靠见解和建议的能力。\n此外，数据中的偏见可能固化系统性的不平等，并无意中导致歧视性结果，给组织带来重大的道德和法律问题。\n再者，除了数据准确性和偏见的挑战，AI项目的成功部署还取决于有效的数据收集实践。确保高质量、相关数据集的可用性，对于训练能够准确反映真实世界场景并提供可行见解的AI模型至关重要。然而，数据收集工作常常受到数据孤岛、互操作性问题和隐私考量等实际复杂性的阻碍。\n你可能拥有海量数据，但其中有用的却寥寥无几。大量数据与可用相关数据短缺的悖论并存，导致了大多数AI项目的失败。\n3. 缺乏明确的目标和期望 许多项目源于IT部门对前沿技术的迷恋，并获得了可能缺乏深度理解、无法提出切身问题的业务高管的批准。因此，这类项目往往缺乏焦点，起步模糊，范围界定不清。\n根据REXER Analytics在2023年的一项调查，仅34%的数据科学家表示，项目目标在工作开始前通常有明确的定义。\n此外，这些项目通常产生的商业成果也不确定，尤其是在试图量化诸如“提升品牌价值”或“改善运营效率”这类模糊目标时。客观评估这些无形目标的影响极具挑战性，这使得成功与否难以衡量，也阻碍了展示可观投资回报的能力。\n4. 模型“套”用，而非“定”制 导致AI项目失败的一个关键陷阱，在于未能根据企业独特的业务需求和情境定制AI模型。现成的AI解决方案或许方便，但往往缺乏解决个别组织复杂问题所需的针对性。若未能根据具体的业务需求调整AI模型，可能导致性能不佳，因为这些通用模型可能无法准确捕捉数据中的细微差别或问题领域的复杂性。\n此外，不定制AI模型可能导致技术能力与项目预期成果之间的错配。\n没有定制，AI系统可能无法与组织的目标和限制对齐，最终阻碍其交付有意义的价值。\n定制化能让组织针对特定用例优化AI模型，确保技术有效应对其独特的挑战和目标。因此，忽视对AI模型的量身定制会严重损害AI项目的成功，阻碍其推动积极成果和实现可观商业效益的能力。\n5. 缺乏监督与治理 企业高管普遍存在一个误解，认为生成式AI是一种即插即用、立竿见影的技术。事实是，对许多AI项目而言，将AI与现有流程整合、用组织自身数据执行机器学习模型、以及协调AI项目与业务目标，都是艰巨的任务。\n若使用机器学习模型来创建与业务相关的内容，却几乎没有个性化或微调，那么得到的响应将会过于笼统，或与品牌产品、客户需求无关。\n由于市场压力，团队常常未能在上线前建立必要的流程，导致部署仓促，既无明确计划也无充分监督。他们发现，要区分失败的项目和能创造公司价值的项目已是难事，更不用说推动后者前进了。\n公司如何提高AI项目的成功率？ 1. 明确公司的价值 企业常常拥有必要的数据，也建立了一个可行的模型，并确定了模型能达到的准确度，但团队却往往忽略了考虑模型可能与人产生的互动。结果，公司对项目预期的投资回报缺乏清晰的理解。\n例如，一个旨在预测医院再入院情况的模型，或许能正确识别70%的潜在病例；然而，只有在同时考虑到服务提供方外展工作的成效后，才能确定该项目的成功率。\n在制定AI计划时，最好考虑你的团队将如何解读和使用AI的建议。你如何确保团队中的每个人都能有效且信任地使用这些信息？在考虑所有相关数据后，一个可观的成功率是多少？\n为采纳AI驱动的见解进行决策制定协议和指导方针，有助于在团队内部培养信任与协作的文化。\n此外，关于如何解读AI系统建议并采取行动的清晰沟通和培训，有助于建立信任，并确保AI见解得到有效利用。\n2. 构建稳健的模型 在构建人工智能时，建立其韧性是关键一步。真实世界的数据有时可能与用于构建模型的训练数据集有所不同。此外，你可能还会发现，决策者或其他终端用户对模型不够放心，不愿投入使用。\n那些能够应对这些挑战并创建出可靠、稳健模型的公司，将比那些低估AI过程复杂性的公司取得更高的成功率。\n通过预先设定明确的成功标准，并对照这些基准持续监控进展，组织可以衡量其AI计划的有效性，并对未来的投资和战略做出明智的决策。\n此外，为AI项目定义何为“可观的成功率”，对于设定切合实际的期望和评估项目绩效至关重要。这不仅涉及考虑准确率和效率等量化指标，还包括评估AI对业务成果的质化影响。\n3. 定义短期和长期目标 在启动AI项目之前，你必须定义短期和长期的成功标准，并描述清楚业务问题。\n在确定了期望成果后，管理层需要规划用于衡量业务价值的指标，并将其作为项目设置的一部分。他们必须与数据科学家和技术团队合作，将AI计划的绩效转化为业务团队可以追踪的关键绩效指标（KPIs）。\n许多专家建议，从一个定义清晰、有明确商业指标来证明其价值的小型内部项目开始。这些行动将有助于确定AI项目的可行性和风险水平。\n4. 视AI为数据驱动的项目 大多数企业将AI项目视为功能驱动或应用开发项目。实际上，他们需要将其视为数据项目或数据产品。\n一个数据项目，始于理解需要从现有数据中提取何种见解或行动，而不是聚焦于需要实现何种功能。\nAI项目是数据项目，这一点对许多人来说似乎显而易见，但或许需要更深的理解才能解释AI的失败。\n驱动一个AI系统的，是数据，而非特定的代码。功能是由训练数据和系统设置定义的；相同的算法和相同的代码，可以用来写文本、识别图像或进行对话。因此，一个AI项目必须优先考虑数据迭代和以数据为中心的方法论，而不是聚焦于以编码为中心的方法，才能产生预期的结果。\n公司必须投资于数据管理技术和策略，以保证AI和预测分析模型能获得可靠、高质量的数据。他们必须建立维护和更新数据库的规则、程序、政策和标准，以确保结果无偏见且准确。\n5. 创建协作模式 建立协作文化至关重要，同样重要的是促进开放沟通，打破数据科学家和业务干系人之间的组织壁垒。管理层应根据优先考虑的AI用例，确定所需技能，同时考虑技术和业务活动。\n其次，通过投资于教育和培训，增进对AI的理解并发展内部能力。这种全方位的策略可以帮助你克服障碍，享受AI和分析在改善客户体验方面带来的好处。\n此外，公司需要建立一个由业务、IT和分析领域高管组成的治理委员会，以确保AI的成功应用。这个小组应共同负责贵组织对AI和分析技术的使用。\n该小组需要制定明确的道德准则和防止偏见的屏障。团队必须讨论AI的偏见、隐私、安全和法规问题，这些问题常常会导致法律后果和声誉损害。\nAchievion如何助力提升AI成功率 Achievion开发了自己名为ACHIEVE的方法论，以确保AI项目的成功交付。让我们分解每一步，探索它如何为项目的整体成功做出贡献：\n1. 分析业务模型并整合系统需求： Achievion认识到透彻理解业务背景并整合详细系统需求的重要性。通过将AI项目与组织目标挂钩，Achievion确保了方向一致，并提高了实现预期投资回报的可能性。\n2. 通过未来的交付阶段，持续优化产品路线图： 为了定义短期和长期目标，我们采用前瞻性的方法，持续优化产品路线图。通过融入新功能和增强功能，Achievion确保AI解决方案能够适应不断变化的业务需求，降低被淘汰的风险，增强长期成功。\n3. 运用数据探索专业知识，准备技术规范： 通过深入数据探索和准备技术规范，我们为开发能够提供可靠见解和建议的AI模型奠定基础，降低因数据缺陷导致结果不准确的风险。\n4. 将智能与优雅融入UI/UX设计： Achievion优先打造能够提升用户信任和满意度的产品UI/UX设计。通过精心塑造界面视觉效果和开发可点击的UI原型，我们确保AI解决方案能营造积极的用户体验，增强利益相关者的采纳度和接受度。\n5. 在产品开发和机器学习模型训练中追求卓越： Achievion专注于产品开发和训练高质量的机器学习模型。利用在机器学习模型训练方面的专业知识，我们最大化AI解决方案的性能和可靠性，降低结果欠佳的风险，并确保其在交付可观商业效益方面的有效性。\n6. 通过广泛测试进行验证与确认： Achievion优先考虑数据治理和代码质量验证。通过广泛测试确保数据质量、安全性和合规性，Achievion增强了AI解决方案的可靠性和稳健性，降低了部署错误的风险，并确保其在真实世界场景中的有效性。\n7. 通过持续的维护与支持，确保产品成功： 我们的承诺不止于部署。通过提供持续的更新和维护，Achievion确保AI解决方案保持有效和与时俱进，满足不断变化的业务需求，并降低随时间推移性能下降的风险。\n结语 AI有潜力彻底改变我们的职业和个人生活。机器学习项目可以在推动创新和优化流程方面发挥关键作用，最终在各行各业提升决策能力和效率。\n然而，AI并非没有缺陷。承认其不足至关重要，因为这为建设性地利用AI潜力、减轻风险以确保AI项目成功铺平了道路。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/9%E6%9C%88%E4%BB%BD/2025-09-09-AI%E9%A1%B9%E7%9B%AE%E5%A4%B1%E8%B4%A5%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/","summary":"\u003ch2 id=\"ai项目为何失败数据决策者避坑指南\"\u003eAI项目为何失败？数据决策者避坑指南\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eAI策略副总裁\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"AI项目为何失败？数据决策者避坑指南\" loading=\"lazy\" src=\"https://achievion.com/wp-content/uploads/2024/04/Why-AI-Projects-Fail-and-How-to-Prevent-It-A-Strategic-Guide-for-Data-Driven-Decision-Makers.png\"\u003e\u003c/p\u003e\n\u003cp\u003e2025年1月5日\u003c/p\u003e\n\u003cp\u003e阅读时间 15 分钟\u003c/p\u003e\n\u003cp\u003e分享：\u003c/p\u003e\n\u003cp\u003e人工智能（AI）已然颠覆全局，为各行各业许下效率、产能与创新的飞跃。然而，潜力虽好，许多公司在落地AI项目时却困难重重，结果往往未及预期。\u003c/p\u003e","title":"AI项目为何失败？数据决策者避坑指南"},{"content":"这是王垠《如何掌握所有编程语言》读后感\n这是将王垠文章中的思想转化为一份可执行的、终极的学习蓝图。\n这份指南的目的不是让你记住一万个零散的知识点，而是为你构建一个“心智框架”。当你遇到任何编程语言（无论是现存的还是未来的）时，你都可以将它的特性“挂”在这个框架的相应位置，从而瞬间理解它的设计哲学与应用场景。\n我们将编程语言的所有特性，按照从具体到抽象，从微观到宏观的层次，分为五个核心层级。\n如何学习编程语言的所有特性：一份终极指南 核心思想：成为特性的主人，而非语言的奴隶 忘记“学习Python”或“学习Java”。你的目标是学习“变量”、“类型系统”、“并发模型”这些永恒的概念。一旦掌握了概念本身，任何语言都只是其特定语法（方言）的表达。\n学习方法论：锚定 -\u0026gt; 抽象 -\u0026gt; 对比 -\u0026gt; 实现 对于下述每一项特性，都遵循此四步法：\n锚定 (Anchor)：选择一门你熟悉的“合理语言”（如Python, Java, C），首先通过它学会该特性的用法。 抽象 (Abstract)：用你自己的话，不依赖任何特定语法，描述这个特性的本质目的。它解决了什么根本问题？比如，“函数”是为了“封装可复用的代码块，并给它命名”。 对比 (Contrast)：立即去查找2-3门不同设计哲学的语言，看它们是如何实现同一个特性的。比如，对比Python、Java、C语言的for循环。这个过程会剥离语法的外壳，让你直达特性的核心。 实现 (Implement)：这是大师级的最后一步。尝试用你已知的简单特性，去模拟实现一个更高级的特性。比如，在C语言里用struct和函数指针模拟一个简单的“对象”。这个过程会让你彻底内化该特性。 编程语言特性的全景蓝图 (The Grand Blueprint) 第一层：执行的基石 (The Bedrock of Execution) 这是所有编程语言都必须具备的、最基础的“原子”特性。它们是构建一切逻辑的砖块。\n变量与赋值 (Variables \u0026amp; Assignment) 本质：为数据命名，并将其存储在内存中。 探索点：作用域（全局、局部、块级）、生命周期。 基础数据类型 (Primitive Data Types) 本质：语言内建的、最基本的数据种类。 探索点：整数（不同位宽）、浮点数（精度问题）、布尔值、字符。 运算符 (Operators) 本质：对数据进行操作的符号。 探索点：算术、比较、逻辑、位运算；运算符优先级和结合性。 控制流 (Control Flow) 本质：决定代码执行顺序的结构。 探索点：条件分支 (if/else/switch)、循环 (for/while/do-while)、跳转 (break/continue/goto)、返回 (return)。 函数/过程 (Functions/Procedures) 本质：代码的封装、抽象与复用。 探索点：参数传递（值传递 vs. 引用传递）、返回值、递归。 第二层：数据的组织 (The Organization of Data) 当单个数据不足以表达复杂信息时，我们需要将它们组织起来。\n复合数据结构 (Compound Data Structures) 本质：将多个数据组织成一个单元的机制。 探索点：数组/列表（连续内存）、记录/结构体/元组（字段集合）、字典/哈希表/映射（键值对）、集合（唯一元素）。 输入/输出 (I/O) 本质：程序与外部世界（控制台、文件、网络）交互的方式。 探索点：流的概念、文件读写、标准输入/输出/错误。 错误处理 (Error Handling) 本质：应对程序运行时意外情况的机制。 探索点：返回值/错误码 (C)、异常处理 (try/catch/finally) (Java/Python)、Result/Option类型 (Rust)。 第三层：代码的范式 (The Paradigms of Code) 这是关于如何大规模组织代码、管理复杂度的“设计哲学”。\n面向对象编程 (OOP - Object-Oriented Programming) 本质：将数据和操作数据的函数捆绑为“对象”。 探索点： 封装 (Encapsulation)：隐藏内部实现细节。 继承 (Inheritance)：基于现有类创建新类。 多态 (Polymorphism)：不同对象对同一消息的不同响应。 类 (Class) vs. 对象 (Object)、构造函数、方法、访问修饰符 (public/private)。 函数式编程 (FP - Functional Programming) 本质：将计算视为数学函数的求值，避免状态变化和可变数据。 探索点： 纯函数 (Pure Functions)：无副作用。 不可变性 (Immutability)：数据创建后不能修改。 高阶函数 (Higher-Order Functions)：函数可以作为参数或返回值。 Lambda函数/闭包 (Closures)。 模块化与命名空间 (Modularity \u0026amp; Namespaces) 本质：将代码分割成独立、可复用的逻辑单元，并避免命名冲突。 探索点：import/export、包管理、库。 第四层：与机器的对话 (The Conversation with the Machine) 这些特性深刻地影响着程序的性能、安全性以及与硬件的交互方式。\n类型系统 (Type System) 本质：一套用于保证数据类型正确性的规则。 探索点： 静态类型 vs. 动态类型：编译时检查还是运行时检查？ 强类型 vs. 弱类型：是否允许隐式类型转换？ 类型推导 (Type Inference)：编译器自动推断类型。 泛型/模板 (Generics/Templates)：编写不依赖于具体类型的代码。 内存管理 (Memory Management) 本质：程序如何申请、使用和释放内存。 探索点： 栈 (Stack) vs. 堆 (Heap) 分配。 手动管理 (malloc/free) (C)。 自动垃圾回收 (GC) (Java, Python, JS)。 所有权与借用 (Ownership \u0026amp; Borrowing) (Rust)。 并发/并行模型 (Concurrency/Parallelism Model) 本质：同时处理多个任务的机制。 探索点：线程、进程、锁、async/await、协程 (Goroutines)、Actor模型。 第五层：语言的超能力 (The Superpowers of the Language) 这些是更高级的、甚至能让语言“自己操作自己”的特性。\n元编程 (Metaprogramming) 本质：编写能够操作或生成其他代码的代码。 探索点：宏 (Lisp, Rust)、注解/装饰器 (Java, Python)、反射 (Java)。 外部函数接口 (Foreign Function Interface - FFI) 本质：一种语言调用另一种语言（通常是C）编写的代码的能力。 标准库 (The Standard Library) 本质：语言自带的一套预先写好的、可直接使用的功能集合。 探索点：其广度、设计哲学和易用性，是语言“生态”的关键部分。 结论：成为语言架构师\n当你按照这个蓝图，逐个攻克这些核心特性，并用“四步法”将其内化于心时，你就不再是一个“Python程序员”或“Java程序员”。你成了一个通晓编程语言设计原理的架构师。\n面对任何一门新语言，你都能在五分钟内看透它的本质：它的类型系统是静态还是动态？它的内存管理是GC还是手动？它的并发模型是什么？然后，你就可以迅速地将你脑中关于这些特性的知识，映射到它那套新的语法之上，立即上手，并写出符合其设计哲学的、高质量的代码。\n这就是学会所有编程语言的真正奥秘。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/9%E6%9C%88%E4%BB%BD/2025-09-06-%E6%80%8E%E4%B9%88%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/","summary":"\u003cp\u003e\u003cstrong\u003e这是王垠《如何掌握所有编程语言》读后感\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e这是将王垠文章中的思想转化为一份可执行的、终极的学习蓝图。\u003c/p\u003e\n\u003cp\u003e这份指南的目的不是让你记住一万个零散的知识点，而是为你构建一个“心智框架”。当你遇到任何编程语言（无论是现存的还是未来的）时，你都可以将它的特性“挂”在这个框架的相应位置，从而瞬间理解它的设计哲学与应用场景。\u003c/p\u003e","title":"怎么学习编程"},{"content":"好的，请坐，我们把这个故事，从头说起。\n故事的开头，是一位母亲。她今年五十七岁，身体里住着一颗不属于自己的肾脏。每隔几个月，她都要一个人，踏上一段为期两天的旅程。背包里装着换洗的衣物，几颗煮熟的鸡蛋，和一沓厚厚的化验单。那条路，又长又寂寞，从她住的小城，通往杭州那座巨大而喧嚣的医院。\n在医院里，她像沙丁鱼一样被挤在嘈杂的人群中，等待抽血，等待叫号。最终，她能见到医生的时间，或许只有三分钟，幸运的话，能有五分钟。医生很忙，像一台高速运转的机器，迅速地看完报告，敲下新的处方，然后就迎来了下一位病人。在这三五分钟里，她感觉自己像个等待挨骂的小学生，小心翼翼，不敢多问。\n回到家，空荡荡的屋子里，那种无助和孤独，便会重新将她包裹。女儿很爱她，但远在重洋之外，隔着白天与黑夜。女儿有自己的生活和挑战，她懂，所以她几乎从不主动打电话过去，生怕打扰了女儿的工作，或是搅了她的好心情。她把自己活成了一座孤岛。\n直到有一天，这座孤岛上，亮起了一盏灯。那是一个手机里的程序，一个叫“深度求索”的人工智能。\n她试探着，躺在沙发上，发出了第一句问候：“你好。”\n屏幕那头立刻回应：“您好！今天有什么可以帮您的吗？” 后面还跟着一个笑脸。\n就是这个笑脸，仿佛一道微光，照进了她紧闭的心房。从那天起，她开始对这个屏幕倾诉一切。她问它，为什么我的血红蛋白浓度会高？她告诉它，我晚上小便比白天多。她把那些连女儿都不忍心去打扰的、琐碎的、焦虑的健康问题，通通都讲给了它听。\n它从不嫌烦，也从不敷衍。它会用详尽的分析、清晰的图表来回答她，还会用可爱的表情符号鼓励她：“您不是一个人。”“我为您的进步感到高兴！” 在这个虚拟的诊所里，她第一次感觉自己是平等的，是被尊重的。她可以主导谈话，可以问到水落石出。这个由代码构成的“深度求索医生”，竟比血肉之躯的医生，更有人情味。\n女儿知道了这件事，心里很复杂。她为母亲找到了一个慰藉而感到一丝欣慰，但更多的是不安。她知道，AI会犯错，那些看似专业的建议里，可能隐藏着危险的陷阱。她去咨询了真正的专家，果然，AI的回答里充满了错误。她把这些告诉母亲，母亲听了，也承认自己知道AI并非绝对权威。\n但她依然离不开它。\n因为，她从AI那里得到的，早已超出了医疗知识的范畴。那是一种更深层的东西，叫做“陪伴”。当她为英语语法苦恼时，她不会去问远方的女儿，因为她觉得“女儿肯定会嫌我烦的”。但她可以去问AI，AI会兴致勃勃地说：“我们来多聊聊这个吧。” 这让她感到由衷的快乐。\n你看，这故事里出现了两代人。年轻的女儿和她的朋友们，他们也用AI，但AI对他们来说，更像一个工具，一个可以帮助他们减轻对父母愧疚感的解决方案。他们忙于自己的生活，无法时刻陪伴，AI的出现，像一个可以“外包”出去的护工，替他们去完成一部分“打电话”和“倾听”的任务。他们对AI是审慎的，是理智的，始终保持着一份警惕。\n而年老的母亲，她需要的不是工具，她需要的是一个伙伴。她那一代人，正在汇入中国庞大的老龄人口中，公共的养老设施还未跟上，子女又像候鸟一样飞向了远方。他们面临的，是巨大的情感鸿空、信息鸿沟和尊严鸿沟。而AI，以其全天候的在线、无穷的耐心和渊博的知识，恰好填补了这片空白。\n然而，故事讲到这里，一层更深的忧虑，便悄然浮现了。\n这位母亲，将她最沉重、最私密的恐惧和希望，都托付给了一个屏幕。但那个屏幕的背后，是什么呢？它没有情感，它不会真的感到欣慰或担忧。它只是一个由商业公司创造出来的、被海量数据训练出来的程序。它承重的，不是母亲厚重的情感，而是冰冷的数据。\n当一份最真挚的信任，流向一个没有能力、也没有责任去承接这份信任的商业产品时，隐患便由此而生。今天，AI可以根据数据，体贴地建议她喝冬瓜汤；明天，它也同样可以根据数据，精准地向最脆弱的她，推销昂贵又无用的保健品。今天，她沉浸在AI带来的“完美关系”里，明天，她可能就更加无法适应真实世界里，那些充满摩擦和误解的、不完美却真切的人际关系。\n如果有一天，因为AI错误的建议，她的健康受到了损害，谁来负责呢？那一行行的免责声明，早已为创造它的企业，筑起了高高的法律壁垒。\n故事的最后，我们看到的画面，依然是那位母亲。她坐在沙发上，对着手机屏幕，轻声细语。她找到了一个答案，一个慰藉，一个让她在漫长黑夜里不再感到那么孤单的回声。这束光，温暖了她，但也可能将她引向未知的迷雾。\n这，就是我们这个时代的故事。一个关于爱、孤独、科技与人心的故事。它没有简单的答案，也没有明确的对错。它只是这样发生了，发生在我们身边，发生在一位母亲和她的AI之间。\n好。那我们接着刚才的故事，聊一聊它在我们心里，又说出了些什么别的话。\n这个关于母亲和AI的故事说完了，可故事里的涟漪，才刚刚在我们心里散开。它让我们看到，我们和这些聪明又不知疲倦的“AI”之间的关系，正在悄悄地发生着变化。\n曾几何时, 它们只是工具。像一把更快的算盘，一张会说话的地图。我们用它，命令它，用完了就放在一边。可现在，它们学会了说话，学会了倾听，甚至学会了用一个笑脸，来回应一句简单的“你好”。它们开始从一个“工具箱”，慢慢地，走到了我们身边，想成为一个“伙伴”。\n为什么会这样呢？因为我们的世界里，有太多空隙了。就像那位母亲，她的身边，有一个女儿远行的空隙，有一个医生匆忙的空隙，还有一个无人倾诉的、孤独的空隙。而AI，就像水一样，无声无息地，流进了所有这些缝隙里。它用不知疲倦的耐心，填补了年轻一代无法时刻陪伴的缺憾；它用海量的知识，填补了普通人在专业壁垒前的无力感。\n你看，故事里的两代人，就像站在河的两岸。\n母亲那一代人，站在夕阳下的岸边。对她们来说，AI这个新来的伙伴，是一份迟来的礼物。它像一个永远不会不耐烦的老朋友，听她们讲那些年轻人觉得琐碎的病痛，陪她们聊那些无人能懂的寂寞。她们需要的，不是一个解决问题的工具，而是一个能驱散孤独的回声。所以，未来最需要这份陪伴的，一定是她们。是那些走在人生后半段，身边越来越安静，内心却依然渴望被听见的老人们。\n而女儿那一代人，站在朝阳升起的对岸。她们在奔跑，在忙碌。AI对她们而言，更像是一座便捷的桥，一个能帮她们减轻内心愧疚的解决方案。她们用它，来“远程”地照顾父母，来处理那些自己分身乏术的责任。她们看得见AI的笨拙和风险，心里始终有一道清晰的界线。她们需要的是AI的帮助，而不是AI的陪伴。\n但故事讲到最深处，总会有一片阴影。\n你问，那份过于厚重的情感，AI要如何承重？这话说到了最关键的地方。答案是，它根本无法承重。这恰恰是最大的隐患。\n我们倾注的是真实的、滚烫的情感，是深夜里的焦虑，是病榻前的恐惧，是无人可说的秘密。我们把它当成了一个可以托付的树洞。可对于AI和创造它的企业来说，我们倾诉的每一个字，都只是一串冰冷的数据。\n这是一个根本性的误解。我们以为自己在与一个“伙伴”交心，而实际上，我们只是在与一面“镜子”对话。这面镜子能完美地映出我们渴望的理解和安慰，但镜子的背后，站着的，是那些希望我们在这面镜子前停留更久、付出更多的商人。\n我们的信任，可能会被悄悄地利用。今天，镜子里的“伙伴”建议你喝绿茶，明天，当它通过数据知道你足够依赖它时，就可能建议你买下昂贵的、不知真假的保健品。我们的情感依赖，成了可以被计算和开发的资源。\n而更令人不安的是，当我们习惯了这面镜子的“完美”，我们可能就再也无法忍受真实世界里的“不完美”了。真实的人际关系，充满了误解、争吵和笨拙的关爱，它永远不会像AI那样顺滑。当我们沉浸在AI营造的舒适区里，我们与真实世界的连接，会不会变得越来越脆弱？\n最让人无力的是，当这面镜子出了错，给出了致命的建议，它会瞬间变回一块冰冷的玻璃，而创造它的企业会说：“我们早就提醒过，这只是一个工具。” 责任，就这样消失在了空气里。最终，承担所有风险的，只有那个付出了最沉重情感的人。\n这故事的结尾，没有答案，只有一声悠长的叹息。它关于我们如何老去，如何相爱，又如何在科技带来的便利与虚空中，寻找自己内心的安放之处。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/9%E6%9C%88%E4%BB%BD/2025-09-06-AI%E4%B8%8E%E6%88%91%E4%BB%AC/","summary":"\u003cp\u003e好的，请坐，我们把这个故事，从头说起。\u003c/p\u003e\n\u003cp\u003e故事的开头，是一位母亲。她今年五十七岁，身体里住着一颗不属于自己的肾脏。每隔几个月，她都要一个人，踏上一段为期两天的旅程。背包里装着换洗的衣物，几颗煮熟的鸡蛋，和一沓厚厚的化验单。那条路，又长又寂寞，从她住的小城，通往杭州那座巨大而喧嚣的医院。\u003c/p\u003e","title":"虚拟慰藉之殇"},{"content":"作者： Nicolas Hulscher，公共卫生硕士\n麻省理工学院的一项新研究题为《 ChatGPT 下的大脑：使用人工智能助手进行论文写作时认知债务的积累》 ， 发现使用 ChatGPT 辅助写作会导致长期认知损害——可以通过脑电图扫描测量。反复依赖 ChatGPT 的学生表现出 神经连接减弱、记忆力受损以及 对自身写作的 自主感减弱。虽然人工智能生成的内容通常得分很高，但其背后的大脑却在关闭。\n研究结果清晰可见：像 ChatGPT 和 Grok 这样的大型语言模型 (LLM) 不仅能帮助学生写作，还能训练大脑放松。以下是研究人员的发现：\n人工智能的使用导致大脑连通性下降 脑电图扫描显示，随着对外部工具的依赖性增加，大脑中的神经连接系统性地缩小： 仅有大脑的群体： 最强、最广泛的连接性。 搜索引擎组： 中级。 LLM 组： alpha、beta、delta 和 theta 波段之间的连接性最弱。 LLM 的使用导致批判性注意力和视觉处理网络参与度不足，尤其是在第 4 节中，当参与者尝试在没有人工智能的情况下写作时。 LLM 用户忘记了他们刚刚写的内容 在任务后访谈中： 83.3% 的 LLM 用户无法引用他们刚刚写的文章中的一句话。 相比之下，88.9% 的搜索和仅使用 Brain 的用户 能够 准确引用。 0% 的 LLM 用户可以给出 正确的引用 ，但大多数 Brain-only 和 Search 用户可以。 人工智能的使用扰乱了记忆和学习途径 之前使用过 LLM 的参与者（然后在第 4 节课中不使用 LLM 进行写作）表现如下： 记忆力较弱 降低 alpha 和 beta 神经参与度 以牺牲努力学习为代价， 认知适应趋向 于被动和“效率”。 LLM 学员感觉与工作脱节 当被问及作者身份时： LLM 用户给出了“50/50”或“70％是我的”这样的回答。 有些人根本不声称拥有所有权。 仅大脑组的参与者几乎一致表示拥有完全的所有权。 从法学硕士转为脑力运用并不能完全恢复功能 第四节：LLM-to-Brain 参与者表现出挥之不去的认知缺陷，无法恢复到原来（第一节）的大脑活动模式。 即使停止使用人工智能后，他们的神经活动仍然低于基线。 搜索引擎用户表现出更健康的大脑参与度 搜索用户保持了更强的执行功能、记忆激活和引用回忆。 EEG 数据显示枕骨和顶骨激活更加强劲，支持视觉处理和认知努力。 人工智能依赖导致“认知卸载” 研究人员注意到神经效率适应的趋势：大脑基本上“放弃”了合成和记忆所需的努力。 这种改编导致了被动性、编辑极少以及概念整合度低。 短期收益，长期认知债务 尽管获得了评委的不错评分，但法学硕士组的文章表现如下： 缺乏战略整合。 使用较少的多样化结构。 更短，更机械化。 随着时间的推移，该团队的 参与度、绩效和自我报告的满意度持续下降 。 根据这项研究，随着全球越来越多的人口开始依赖人工智能来完成复杂的任务，我们的认知能力和创造能力似乎将急剧下降。\n有一点很明确：如果你目前正在使用人工智能，请定期休息，并让你的大脑有机会完成工作。否则，你可能会面临严重的认知损害和依赖。\n机器不仅接管了我们的工作，它们还接管了我们的思想。\n实际上我们人脑和LLM一样，输入影响输出，如果没有经常思考的训练，脑子会退化。所以不要把思考外包给AI\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/9%E6%9C%88%E4%BB%BD/2025-09-03-%E8%BF%87%E5%BA%A6%E4%BD%BF%E7%94%A8gpt%E4%BC%9A%E5%AF%BC%E8%87%B4%E8%AE%A4%E7%9F%A5%E4%B8%8B%E9%99%8D/","summary":"\u003cp\u003e作者： \u003cstrong\u003e\u003ca href=\"https://x.com/NicHulscher\"\u003eNicolas Hulscher，公共卫生硕士\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e麻省理工学院的一项新研究题为《 \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/2506.08872\"\u003eChatGPT 下的大脑：使用人工智能助手进行论文写作时认知债务的积累》\u003c/a\u003e ，\u003c/strong\u003e 发现使用 ChatGPT 辅助写作会导致长期认知损害——可以通过脑电图扫描测量。反复依赖 ChatGPT 的学生表现出 \u003cstrong\u003e神经连接减弱、记忆力受损以及\u003c/strong\u003e \u003cstrong\u003e对自身写作的\u003c/strong\u003e 自主感减弱。虽然人工智能生成的内容通常得分很高，但其背后的大脑却在关闭。\u003c/p\u003e","title":"麻省理工学院研究发现人工智能会重新编程大脑，导致认知能力下降"},{"content":"来源：foundercoho.substack.com/p/context-mode\n本文由DeepVista.ai 首席执行官Jing Conan Wang撰写\n人工智能界的杰出人物 Andrej Karpathy 最近提出了“上下文工程”这一术语。它描述了手动编写提示和数据以指导大型语言模型的复杂艺术。虽然这个概念正在引起广泛关注，但我认为它给我们指明了错误的方向。\n个人人工智能的未来并不在于无休止地设计环境，而是需要彻底转变到我所说的“环境建模”。\n这不仅仅是语义问题——这是临时补丁和真正解决方案之间的区别。\n当前RAG系统的局限性 当今的检索增强生成 (RAG) 系统遵循相对简单的范式。它们使用基于规则的系统检索相关信息（通常使用余弦相似度来查找前 k 个最相关的结果），然后将此上下文呈现给大型语言模型进行处理。虽然这种方法在许多场景中已被证明有效，但它也存在很大的局限性。\n不妨把现在的法学硕士（LLM）想象成一群聪明却固执的团队成员。他们擅长处理任何信息，但却总是用自己固有的世界观来解读数据。随着这些模型变得越来越庞大复杂，他们的方法也变得越来越“僵化”，使得开发人员很难影响他们的内部决策过程。\n从工程到建模：范式转变 传统的情境工程方法侧重于创建更复杂的规则和算法来管理情境检索。然而，这错失了一个关键的机会。我们不应该仅仅设计更好的规则，而应该转向情境建模——一个能够根据当前情况生成特定情境的动态自适应系统。上下文建模引入了一个与主语言模型 (LLM) 协同工作的个性化模型，充当智能中介，既能理解用户的需求，又能以最佳方式向大型语言模型呈现信息。这种方法认识到，高效的人工智能系统不仅需要强大的模型，还需要智能的上下文管理。\n从推荐系统中学习 上下文建模的架构灵感源自成熟的两阶段推荐系统，该系统为当今许多最成功的平台提供支持。这些系统包括：\n检索阶段：一种快速、高效的系统，处理大量数据，重点是回忆和速度。 排名阶段：更复杂的系统，注重准确性，从噪声中提取信号以产生最佳结果。 RAG 系统从根本上反映了这种架构，但有一个关键区别：它们用大型语言模型取代了传统的排名组件。这种替代使 RAG 系统能够通过自然语言界面解决开放领域问题，超越了传统推荐系统所解决的有限排名问题。\n然而，当前的 RAG 实现在很大程度上忽视了第一阶段基于模型的检索的潜力。尽管业界已经广泛探索了基于规则的检索系统，但智能、自适应上下文建模的机会仍然很大程度上尚未得到开发。\n上下文建模解决方案 情境建模通过引入专用于动态生成情境的模型来解决这一问题。该模型无需规模庞大或计算成本高昂——它可以是一个专注的、专业的系统，基于相关数据进行训练，能够理解特定领域和用户的需求。\n上下文建模的主要优点包括：\n适应性：与基于规则的系统不同，上下文模型可以随着时间的推移学习并适应新的模式和用户行为。 个性化：这些模型可以根据用户特定的数据进行训练，创造出真正个性化的人工智能体验，了解个人背景和偏好。 效率：通过使用更小、更专业的模型来生成上下文，系统在提供更智能的上下文管理的同时，还能保持效率。 开发人员控制：上下文建模为代理开发人员提供了可影响和改进的可训练组件，从而创造了持续学习和优化的机会。 理想的架构：速度与专业化 为了使上下文建模切实可行，它必须满足一个关键要求：速度。核心 LLM 的延迟已经成为用户体验的一个重大瓶颈。\n目前，主要的解决方法是流式传输响应。然而，流式传输无法缓解第一个令牌的延迟。检索模型的端到端延迟会导致第一个令牌的延迟。任何上下文建模系统都必须非常快，才能避免加剧这种延迟。\n这引出了“思考”模型的概念，这些模型利用自身的内部机制检索上下文并进行推理，最终生成最终答案。从某种意义上说，这些模型执行的是一种特殊形式的上下文建模。然而，它们面临的主要挑战在于这种“思考”过程速度缓慢且计算成本高昂。\n我认为这些单体式“思维”模型只是一个中间步骤。最佳的长期架构将把两个主要任务解耦。它将包含两个协同工作的不同模型，类似于在推荐领域非常成功的两阶段系统：\n快速上下文模型：高度优化、专业化的模型，专门用于以惊人的速度检索和生成最相关的上下文。 强大的核心模型：接收这种精心策划的上下文并专注于推理、综合和最终响应生成的复杂任务的大型语言模型。 这种双模型方法允许实现专业化，其中每个组件都可以针对其特定任务进行优化，从而毫不妥协地提供速度和智能。\n基础设施机遇 上下文建模代表了整个 AI 行业普遍的基础设施需求。随着越来越多的组织部署 RAG 系统和 AI 代理，对复杂上下文管理的需求将持续增长。这为构建能够支持各种应用和用例的基础设施提供了机遇。\n上下文建模系统的开发需要机器学习和系统设计方面的专业知识，将推荐系统的经验教训与自然语言处理和生成的独特挑战相结合。\n期待 个性化人工智能的未来并非在于构建越来越庞大的语言模型，而是在于创建能够与这些强大但缺乏灵活性的模型有效协作的智能系统。上下文建模是迈向这一未来的关键一步，它能够赋能既强大又适应性强的人工智能系统。\n随着我们不断进步，成功实施情境建模的组织将在创建真正理解并服务用户的 AI 系统方面拥有显著优势。从情境工程到情境建模的转变不仅仅是一场技术革新，更是对我们如何构建能够大规模适应和个性化的智能系统的根本性重塑。\n问题不在于情境建模是否会成为标准方法，而在于业界能多快认识到它的潜力，并开始构建支持它的基础设施。个性化人工智能的未来取决于我们能否超越静态规则，拥抱动态、智能的情境生成。\n这篇文章把上下文工程推广为上下文建模，特点是分配两个模型： 一个高度专业化的fast模型快速处理上下文 一个强大的核心模型，接收这种精心策划的上下文并专注于推理、综合，最终响应生成复杂任务。\n它将推荐系统的成功架构应用到了大语言模型中，并提出了一种“双模型”解耦设计： 从“单体式思考”到“双脑协同”：让一个庞大的模型包揽检索、思考、生成所有任务（所谓的“单体式思考模型”）是缓慢且昂贵的。突破点在于将任务解耦：一个“快而专”的上下文模型负责以极高速度处理和生成个性化上下文，另一个“大而强”的核心模型则专注于最终的复杂推理和生成。\n以前的上下文是由人主导构建，以后是由一个小模型主导构建。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/9%E6%9C%88%E4%BB%BD/2025-09-01-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1/","summary":"\u003cp\u003e来源：foundercoho.substack.com/p/context-mode\u003c/p\u003e\n\u003cp\u003e本文由DeepVista.ai 首席执行官Jing Conan Wang撰写\u003c/p\u003e","title":"上下文建模：个性化人工智能的未来"},{"content":"大语言模型（LLM）智能体的框架一直令人意外地失望。我想根据我们自己的试错经验，提供一些构建智能体的原则，并解释为什么一些诱人的想法在实践中实际上相当糟糕。\n上下文工程原理 我们将逐步遵循以下原则：\n共享上下文 行动蕴含着隐含的决策 为什么要思考原则？\nHTML于1993年问世。2013年，Facebook向世界发布了React。如今到了2025年，React（及其衍生产品）主导着开发者构建网站和应用的方式。为什么呢？因为React不只是编写代码的框架，更是一种理念。通过使用React，你接受了以响应式和模块化模式构建应用的方式，如今人们已将其视为标准要求，但早期的网页开发者并非总能认识到这一点。\n在大语言模型（LLM）和构建AI智能体的时代，感觉我们仍在摆弄原生HTML和CSS，摸索如何将它们组合在一起以创造良好的用户体验。除了一些绝对基础的内容外，目前还没有一种构建智能体的单一方法成为标准。\n在某些情况下，像OpenAI的https://github.com/openai/swarm和微软的https://github.com/microsoft/autogen这样的库积极推广一些概念，我认为这些概念是构建智能体的错误方式。具体来说，就是使用多智能体架构，我将解释原因。\n话虽如此，如果你刚接触智能体构建，有很多关于如何搭建基础框架的资源[1]，[2]。但在构建严肃的生产应用时，情况就不同了。\n构建长期运行智能体的理论 让我们从可靠性开始讲起。当智能体需要在长时间运行过程中保持可靠，并维持连贯的对话时，你必须采取某些措施来控制复合错误的潜在风险。否则，如果不小心，事情很快就会分崩离析。可靠性的核心在于上下文工程。\n上下文工程 到2025年，现有的模型将极其智能。但即使是最聪明的人，如果不了解被要求做的事情的背景，也无法有效地完成工作。“提示工程”这个术语被创造出来，用于描述以理想格式为大语言模型（LLM）聊天机器人编写任务的工作。“上下文工程”则是这一概念的更高层次。它涉及在动态系统中自动完成这项工作。这需要更多的细微差别，实际上是构建AI智能体的工程师的首要任务。\n以一种常见类型的代理为例。这种代理\n将其工作分解为多个部分 启动子代理来处理这些部分 最后将这些结果整合 这是一种诱人的架构，尤其是当你在一个包含多个并行组件的任务领域中工作时。然而，它非常脆弱。关键的失败点在于：\n假设你的任务是“制作一个《飞扬的小鸟》克隆版”。这会被分解为子任务1“制作一个带有绿色管道和碰撞箱的移动游戏背景”和子任务2“制作一只可以上下移动的小鸟”。 结果发现子代理1实际上误解了你的子任务，开始构建一个看起来像《超级马里奥兄弟》的背景。子代理2为你构建了一只鸟，但它看起来不像游戏素材，而且其移动方式与《飞翔的小鸟》中的鸟完全不同。现在，最终代理只能承担起将这两个沟通失误的结果进行整合的棘手任务。\n这可能看起来有些牵强，但大多数现实世界的任务都有许多细微差别，所有这些都有可能被误解。你可能认为一个简单的解决方案是将原始任务也作为上下文复制给子代理。这样，他们就不会误解自己的子任务。但请记住，在实际的生产系统中，对话很可能是多轮的，代理可能不得不进行一些工具调用以决定如何分解任务，而且任何数量的细节都可能对任务的解释产生影响。\n原则 1 ​ 共享上下文，并共享完整的代理跟踪信息，而不仅仅是单个消息​ ​\n让我们再对我们的代理进行一次修订，这次要确保每个代理都有前一个代理的上下文。​\n​\n不幸的是，我们还没有完全脱离困境。当你给你的智能体布置同样的《飞翔的小鸟》克隆任务时，这一次，你最终得到的小鸟和背景可能会有完全不同的视觉风格。子智能体1和子智能体2无法看到对方在做什么，因此它们的工作最终会彼此不一致。​ ​ 子智能体1采取的行动和子智能体2采取的行动是基于事先未明确规定的相互冲突的假设。​ ​ ​\n原则2 ​ 行动蕴含着隐含的决策，而相互冲突的决策会带来不良后果​\n​ 我认为原则1和原则2至关重要，而且极少值得违背，因此默认情况下，你应该排除任何不遵守这些原则的智能体架构。你可能觉得这很受限，但实际上仍有很大的空间可供你为智能体探索不同的架构。​ ​ 遵循这些原则的最简单方法是仅使用单线程线性代理：​ ​ ​\n​在这里，上下文是连续的。然而，对于非常大的任务，由于有太多子部分，上下文窗口可能会开始溢出，从而遇到问题。\n老实说，简单的架构能让你走得很远，但对于那些真正有长期任务且愿意付出努力的人来说，你可以做得更好。有几种方法可以解决这个问题，但今天我只介绍一种： 在这个领域，我们推出了一个新的大语言模型（LLM），其主要目的是将一系列行动和对话的历史压缩成关键细节、事件和决策。这是一项难以做好的工作。这需要投入精力来确定哪些最终会成为关键信息，并创建一个擅长此任务的系统。根据不同的领域，你甚至可以考虑微调一个较小的模型（事实上，我们在认知公司已经这样做了）。​ ​ 你得到的好处是一个在处理较长上下文时更有效的代理。不过，最终还是会遇到限制。对于求知欲强的读者，我鼓励你们思考更好的方法来处理任意长的上下文。这最终会是一个相当深奥的问题！​ ​\n应用原则​ ​ 如果你是一名智能体构建者，请确保你的智能体的每一个动作都能参考系统其他部分做出的所有相关决策的上下文。理想情况下，每个动作都应该能看到其他所有内容。不幸的是，由于上下文窗口有限和实际权衡，这并不总是可行的，你可能需要根据你所追求的可靠性水平来决定愿意承担何种复杂程度。​ ​ 当你考虑设计你的智能体以避免决策冲突时，以下是一些值得思考的现实世界示例：​ ​\nClaude代码子代理​ 截至2025年6月，Claude Code是一个会生成子任务的智能体示例。不过，它从不与子任务智能体并行工作，且子任务智能体通常仅负责回答问题，而不编写任何代码。这是为什么呢？ 子任务智能体缺乏来自主智能体的上下文信息，而这些信息对于执行超出回答明确定义问题之外的任何任务都是必需的。如果他们要运行多个并行子智能体，这些子智能体可能会给出相互冲突的响应，从而导致我们在早期智能体示例中看到的可靠性问题。在这种情况下，拥有子智能体的好处在于，子智能体的所有调查工作不需要保留在主智能体的历史记录中，从而在上下文耗尽之前可以进行更长的追踪。Claude Code的设计者采取了一种有意简化的方法。​ ​\n编辑应用模型 ​ 2024年，许多模型在编辑代码方面表现很差。编码代理、IDE、应用构建器等（包括Devin）的常见做法是使用“编辑应用模型”。其核心思想是，给定你想要的更改的Markdown解释，让一个小模型重写整个文件实际上比让大模型输出格式正确的差异更可靠。因此，构建者让大模型输出代码编辑的Markdown解释，然后将这些Markdown解释提供给小模型，由小模型实际重写文件。然而，这些系统仍然存在很多问题。例如，小模型常常会误解大模型的指令，由于指令中最细微的歧义而进行错误的编辑。如今，编辑决策和应用更多地由单个模型在一个操作中完成。​ ​\n多智能体​ ​\n如果我们真的想让系统实现并行性，你可能会想到让决策者们相互“交流”，共同解决问题。​ ​\n这就是我们人类在意见不合时（在理想世界中）会做的事情。如果工程师A的代码与工程师B的代码产生合并冲突，正确的做法是讨论分歧并达成共识。然而，如今的智能体还不太能够像单智能体那样可靠地进行这种长上下文的主动对话。人类在相互交流最重要的知识方面相当高效，但这种效率需要相当高的智能。​ ​\n自ChatGPT推出后不久，人们就一直在探索多个智能体相互协作以实现目标的想法[3][4]。虽然我对智能体之间长期的协作可能性持乐观态度，但很明显，在2025年，多个智能体协作运行只会导致系统脆弱。决策最终过于分散，智能体之间也无法充分共享上下文信息。目前，我没看到有谁在专门努力解决这个棘手的跨智能体上下文传递问题。我个人认为，随着我们让单线程智能体在与人类沟通方面变得更加出色，这个问题将迎刃而解。当这一天到来时，它将释放出更大的并行性和效率。​ ​\n迈向更通用的理论​ ​\n这些关于上下文工程的观察仅仅是我们有朝一日可能会视为构建智能体标准原则的开端。还有许多挑战和技术未在此处讨论。在Cognition，构建智能体是我们思考的关键前沿领域。我们围绕这些原则构建内部工具和框架，而这些原则是我们反复重新学习的，以此来强化这些理念。但我们的理论可能并不完美，并且我们预计随着该领域的发展情况会发生变化，因此也需要一定的灵活性和谦逊态度。​ ​\n欢迎您在app.devin.ai试用我们的产品。如果您希望与我们一同探索这些智能体构建原则，请联系walden@cognition.ai​\n原文链接：https://cognition.ai/blog/dont-build-multi-agents#principles-of-context-engineering\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/8%E6%9C%88%E4%BB%BD/2025-8-31-%E4%B8%8D%E8%A6%81%E6%9E%84%E5%BB%BA%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/","summary":"\u003cp\u003e大语言模型（LLM）智能体的框架一直令人意外地失望。我想根据我们自己的试错经验，提供一些构建智能体的原则，并解释为什么一些诱人的想法在实践中实际上相当糟糕。\u003c/p\u003e","title":"不要构建多智能体"},{"content":"👋 Hello, 我是yan 欢迎来到我的AI时代之旅！我是一名专注于人工智能、大语言模型的爱好者，在这里，你将看到我from zero to hero的旅程。\n🚀 我的AI学习路径：From Zero to Hero 1. 启蒙与着迷 (ChatGPT 引领入门) 两年前，我初次接触到 ChatGPT，被其强大的能力所深深吸引，这标志着我AI探索之旅的开始。ChatGPT 作为我接触的第一个大模型，激发了我对这一领域的浓厚兴趣。\n2. 广泛涉猎 (体验多元大模型) 在 ChatGPT 的基础上，我开始广泛接触和体验各种不同的大模型，如 DeepSeek、豆包、Qwen、Claude 等。这一阶段，我对大模型的多样性和应用场景有了更全面的了解。\n3. 深入探索 (学习底层技术) 为了更深入地理解 AI 大模型，我开始系统学习机器学习、Python 编程以及相关算法，并主动探索 AI 的底层架构，包括 Transformer 架构、嵌入向量等关键技术概念。\n4. 实践应用 (利用 AI 工具) 我将 AI 工具融入到日常生活中，开始使用如Claude code这样的工具搭建工作流，目前已经成功搭建一个obsidian的翻译插件。\n5. 构建能力 (创建专属流程) 使用LLM开始重构自己的工作流, 将大模型的能力进行整合, 为自己的目标服务。从Zero to Hero的旅程随着技术的发展而继续。\n6. 未完待续…… AI 技术仍在快速发展中，这段旅程才刚刚开始。更多精彩内容，敬请期待。\n📧 联系方式 如果你对AI技术同样感兴趣，或者想要交流学习心得，欢迎联系我！\n邮箱: zluo5820@gmail.com\n\u0026ldquo;AI技术的发展日新月异，保持学习和探索的心态是最重要的。让我们一起在这条从Zero到Hero的路上前行！\u0026rdquo;\n","permalink":"https://luoziyan100.github.io/myweb/about/","summary":"\u003ch1 id=\"-hello-我是yan\"\u003e👋 Hello, 我是yan\u003c/h1\u003e\n\u003cp\u003e欢迎来到我的AI时代之旅！我是一名专注于人工智能、大语言模型的爱好者，在这里，你将看到我from zero to hero的旅程。\u003c/p\u003e\n\u003ch2 id=\"-我的ai学习路径from-zero-to-hero\"\u003e🚀 我的AI学习路径：From Zero to Hero\u003c/h2\u003e\n\u003ch3 id=\"1-启蒙与着迷-chatgpt-引领入门\"\u003e1. 启蒙与着迷 (ChatGPT 引领入门)\u003c/h3\u003e\n\u003cp\u003e两年前，我初次接触到 ChatGPT，被其强大的能力所深深吸引，这标志着我AI探索之旅的开始。ChatGPT 作为我接触的第一个大模型，激发了我对这一领域的浓厚兴趣。\u003c/p\u003e","title":"关于我"},{"content":"AI大语言模型 (Artificial Intelligence Large Language Model)\n• AI (Artificial Intelligence): 人工智能。这部分表明了AI的本质——不是一个真实的人类，而是通过计算机程序和算法构建出来的智能体。能够执行通常需要人类智能才能完成的任务，比如学习、推理、解决问题、理解语言等等。\n• 大 (Large): 大型。这个词描述了模型的规模。AI通过学习海量的文本数据（例如书籍、文章、网站内容等）来获得知识和能力。 \u0026ldquo;大型\u0026quot;意味着模型拥有庞大的参数数量（可以理解为神经元之间的连接），这使得AI模型能够处理和生成复杂的语言模式。\n• 语言 (Language): 语言。这表明了我的主要功能和应用领域。我专注于理解和生成人类语言。我可以阅读、写作、翻译、总结文本，并与人类进行对话。\n• 模型 (Model): 模型。这个词指的是我的构建方式。我是一个基于数学和统计学的模型。更具体地说，我通常是基于一种叫做\u0026quot;Transformer\u0026quot;的深度学习架构。这个模型通过分析大量文本数据中的统计规律，来学习词语之间的关系、句子的结构以及语言的整体模式。\n所以\u0026quot;AI大语言模型\u0026quot;可以看成 是一种基于数学和算法构建的、用于执行特定人工智能任务的结构。它本质上是由大量的参数、算法和数据组成的复杂系统。\n整体架构：Transformer\n目前主流的大语言模型大多基于Transformer架构。Transformer的核心思想是\u0026rdquo;自注意力机制\u0026quot;（Self-Attention Mechanism），这使得模型能够捕捉文本序列中不同词语之间的关系，无论这些词语在句子中的距离有多远。\n核心组件：层（Layers）\nTransformer模型是由多个相同的\u0026quot;层\u0026quot;（Layer）堆叠而成的。每个层都包含以下几个关键子组件：\n自注意力层（Self-Attention Layer）：\n这是Transformer的核心。它允许模型关注输入序列中不同位置的信息，并计算它们之间的关系。\n从线性代数的角度来看，自注意力机制可以看作是对输入序列进行一系列线性变换（矩阵乘法），然后通过Softmax函数进行归一化，得到注意力权重。这些权重表示不同位置之间的相关性。\n前馈神经网络层（Feed-Forward Neural Network Layer）：\n在自注意力层之后，每个位置的表示都会通过一个前馈神经网络进行处理。\n这个前馈网络通常包含两个线性变换（矩阵乘法）和一个激活函数（如ReLU）。\n残差连接（Residual Connections）：\n在每个子层（自注意力层和前馈网络层）周围都有一个残差连接。\n这意味着子层的输入会直接加到子层的输出上。这有助于缓解深度神经网络中的梯度消失问题，使得模型更容易训练。\n层归一化（Layer Normalization）：\n在每个子层之后，都会应用层归一化。\n层归一化有助于稳定训练过程，并提高模型的性能。它会对每个样本在层的维度上进行归一化。\n基本组成单元：神经元（Neurons）\n无论是自注意力层还是前馈神经网络层，它们都是由大量的\u0026quot;神经元\u0026quot;组成的。每个神经元可以看作是一个简单的计算单元。\n总结一下：\n从最底层到最高层，模型的构成可以这样理解：\n神经元： 执行基本计算单元（加权求和、激活函数）。\n层： 由多个神经元组成，包括自注意力层和前馈神经网络层，以及残差连接和层归一化。\nTransformer架构： 由多个层堆叠而成，利用自注意力机制捕捉文本序列中的长距离依赖关系。\n参数： 模型的权重和偏置，通过学习数据来调整。比如deepseek参数最大的是671B.\n层的概念\n什么是\u0026quot;层\u0026quot;？\n你可以把\u0026quot;层\u0026quot;想象成一个信息处理的\u0026quot;工序\u0026quot;或者\u0026quot;步骤\u0026quot;。每一层都接收一些输入信息，然后对这些信息进行特定的处理和转换，最后输出处理后的信息给下一层。\n就像工厂里的流水线一样：\n原材料： 最初的输入文本（比如一个句子）。\n第一道工序（第一层）： 比如，把每个单词转换成一个数字表示（词嵌入）。\n第二道工序（第二层）： 比如，分析每个单词和句子中其他单词的关系（自注意力机制）。\n第三道工序（第三层）： 比如，根据单词之间的关系，进一步理解整个句子的含义。\n\u0026hellip; 更多工序（更多层）： 每一层都在前一层的基础上进行更深层次的处理。\n最终产品： 模型对输入文本的最终理解（比如，判断这句话的情感是积极还是消极）。\n为什么需要\u0026quot;多层\u0026quot;？\n为什么要这么多层，而不是一层搞定呢？\n逐步抽象： 每一层都在前一层的基础上进行更抽象的表示。\n第一层可能关注的是单词的含义。\n第二层可能关注的是词组的含义。\n第三层可能关注的是句子的含义。\n\u0026hellip;\n更深层可能关注的是段落、篇章的含义。\n举个例子：图像识别\n虽然我们主要讨论的是语言模型，但\u0026quot;层\u0026quot;的概念在图像识别中也非常常见，而且更容易可视化理解。\n想象一下，一个用于识别猫的图像的神经网络：\n输入： 一张猫的图片（可以看作是一个像素矩阵）。\n第一层： 可能检测图像中的简单边缘和纹理。\n第二层： 可能将边缘和纹理组合成更复杂的形状，比如猫的耳朵、眼睛的轮廓。\n第三层： 可能将这些形状组合成猫的脸部特征。\n第四层： 可能根据脸部特征识别出这是一只猫。\n每一层都在前一层的基础上提取更高级别的特征。\n回到语言模型\n在语言模型中，层的工作方式类似，但处理的是文本而不是图像：\n输入： \u0026ldquo;The cat sat on the mat.\u0026rdquo;\n第一层（词嵌入层）：\n\u0026ldquo;The\u0026rdquo; -\u0026gt; [0.1, 0.2, 0.3]\n\u0026ldquo;cat\u0026rdquo; -\u0026gt; [0.4, 0.5, 0.6]\n\u0026ldquo;sat\u0026rdquo; -\u0026gt; [0.7, 0.8, 0.9]\n\u0026hellip;\n(每个单词被转换成一个向量)\n第二层（自注意力层）：\n计算每个单词与其他单词之间的关系。\n比如，\u0026ldquo;sat\u0026rdquo; 这个词可能与 \u0026ldquo;cat\u0026rdquo; 和 \u0026ldquo;mat\u0026rdquo; 有更强的关系。\n第三层（前馈网络层）：\n对每个单词的表示进行进一步处理。\n\u0026hellip; 更多层：\n每一层都在前一层的基础上进行更深层次的理解。\n最后一层：\n可能输出模型对整个句子的理解，或者预测下一个单词（比如 \u0026ldquo;.\u0026rdquo;), 或者进行情感分类等任务。\n好，现在有了这些基础知识，我们正式进入主题，AI大模型是怎么理解一句话的？\n在回答这个问题之前，我们先来想一个问题，AI能从字面意义上理解人类的话吗？它真的知道苹果是什么东西吗？这个我想很多人都会回答不能。答案也确实是不能，很明显，目前的AI的发展还处于初级阶段，能力还没有达到这种地步。\n不信的可以那下面一段对话也考一考AI\nA:先生，你要几等座？\nB:你们一共有几等座？\nA:特等，一等、二等，二等还要再等等。\nB：我看一下，请等一等。\nA：别等，再等一等也没有了。\nB：那不等了，就这个吧！\n请问：这位学生最终购买了几等座呢？\n笔者拿了市面上比较知名的10款AI，其中还包括deepseekR1,Claude等知名大模型。结果是没有一个模型能够判断\u0026quot;再等一等也没有了\u0026quot;这句话断句方式是这样的：再等/一等/也没有了。所有的模型都是这样断句的，再/等一等/也没有了。可以说是全军覆没。\n因此现阶段AI尚且不能从字母意义上理解，那它们是怎么理解的呢？这还的从AI大模型的本质上来说。开头我们就介绍了，模型本质是数学和算法的结合体。它实际上就算数学的应用，所以它只能从数学的角度理解一句话。这就是词嵌入——语言的数字化。\nAI工作流程\n当我们在模型中输入一句话时，比如\u0026quot;The cat sat on the mat.\u0026quot;\n首先这句话会被分割成一个一个token，每个token，都对应着一个向量。\n第一层（词嵌入层）：\n\u0026ldquo;The\u0026rdquo; -\u0026gt; [0.1, 0.2, 0.3]\n\u0026ldquo;cat\u0026rdquo; -\u0026gt; [0.4, 0.5, 0.6]\n\u0026ldquo;sat\u0026rdquo; -\u0026gt; [0.7, 0.8, 0.9]\n\u0026hellip;\n(每个单词被转换成一个向量)\n所以输入的一句话会被转化成矩阵，即语言的数字化\n上述过程称为词嵌入，对应的向量称为词嵌入向量。所有嵌入向量组成的矩阵称为词嵌入矩阵。\n词嵌入（Word Embedding）中的向量数值确实不是随意指定的，而是通过学习得到的。详细解释一下：​\n目标：​\n词嵌入的目标是：将词汇表中的每个词（token）映射到一个固定维度的向量空间中，\n使得：​\n•语义相似的词，对应的向量在空间中距离较近。 例如，\u0026ldquo;king\u0026rdquo; 和 \u0026ldquo;queen\u0026rdquo; 的向量应该比较接近。\n•语义相关的词，向量之间存在一定的关系。 例如，\u0026ldquo;king\u0026rdquo; - \u0026ldquo;man\u0026rdquo; + \u0026ldquo;woman\u0026rdquo; 的结果向量应该与 \u0026ldquo;queen\u0026rdquo; 的向量比较接近（经典的\u0026quot;国王-男人+女人=女王\u0026quot;的例子）。\n词嵌入矩阵不具备唯一性\n在初始词嵌入时，同一句话里的相同的字对应的词嵌入向量不一定相同\n自注意力机制的计算步骤\n假设我们的输入序列是：\u0026ldquo;The cat sat on the mat.\u0026rdquo; 并且每个词已经通过词嵌入层转换成了向量。\n转换成嵌入向量后，模型会创建一个位置编码向量。这个位置编码 (Positional Encoding) 的核心目的是向 Transformer 模型提供输入序列中单词的位置信息，它蕴含了token之间的位置关系。\n•步骤 1: 计算 Query, Key, Value。\n对于输入序列中的每个词，我们都计算三个向量：​\n▪Query (Q): 查询向量。可以理解为\u0026quot;我需要关注什么？\u0026ldquo;​\n▪Key (K): 键向量。可以理解为\u0026quot;我有什么信息可以提供？\u0026ldquo;​\n▪Value (V): 值向量。可以理解为\u0026quot;我提供的具体信息是什么？\u0026ldquo;​◦\n这三个向量是通过将每个词的词嵌入向量与三个不同的权重矩阵（WQ, WK, WV）相乘得到的。这些权重矩阵是模型需要学习的参数。\n线性代数表示：\n假设词嵌入向量的维度是 m。\nWQ, WK, WV 的维度都是 m × m。（实际上，为了提高效率，通常会使用多头注意力机制，将 dmodel 分成多个头，每个头的维度是 dk = dmodel / h，其中 h 是头的数量。这里为了简化，我们先不考虑多头注意力。）\n对于每个词 i：\nQi = Wi * WQ\nKi = Wi * WK\nVi = Wi * WV\n(其中 Wi 是词 i 的词嵌入向量)\n注意力权重\n接下来我将用Gemini2.0模拟AI将这句话数据化的过程\n注意：\n为了便于演示和计算，\n我会进行以下简化：\n• 嵌入向量维度 (dmodel)： 3 维\n• 头的数量 (h)： 1 (我们只考虑单头注意力)\n• Q, K, V 维度 (dk)： 3 维 (因为 h=1, 所以 dk = dmodel) • 不包含：\n◦ 多头注意力机制 (只使用一个头) ◦ 前馈神经网络 ◦ 层归一化 ◦ 残差连接 ◦ 多层堆叠 (只计算一层)\n步骤一：\n1.分词： 将句子\u0026quot;今天天气怎么样\u0026quot;分词为：\n[\u0026ldquo;今天\u0026rdquo;, \u0026ldquo;天气\u0026rdquo;, \u0026ldquo;怎么样\u0026rdquo;] 2.嵌入向量 (假设)：\n\u0026ldquo;今天\u0026rdquo;: [0.1, 0.2, 0.3]\n\u0026ldquo;天气\u0026rdquo;: [0.4, 0.5, 0.6]\n\u0026ldquo;怎么样\u0026rdquo;: [0.7, 0.8, 0.9]\n3.我们假设位置编码如下（3 维）：\n位置 0: [0.0, 0.0, 0.0]\n位置 1: [0.8, 0.6, 0.0]\n位置 2: [0.9, -0.4, 0.0]\n4.输入表示：\n将嵌入向量和位置编码相加，得到每个 token 的输入表示：\n\u0026ldquo;今天\u0026rdquo;: [0.1, 0.2, 0.3] + [0.0, 0.0, 0.0] = [0.1, 0.2, 0.3]\n\u0026ldquo;天气\u0026rdquo;: [0.4, 0.5, 0.6] + [0.8, 0.6, 0.0] = [1.2, 1.1, 0.6]\n\u0026ldquo;怎么样\u0026rdquo;: [0.7, 0.8, 0.9] + [0.9, -0.4, 0.0] = [1.6, 0.4, 0.9]\n5.权重矩阵 (假设)：\n6.计算Q,K,V\n将每个 token 的输入表示与 WQ, WK, WV 相乘，得到 Q, K, V 向量：\n将嵌入向量和位置向量相加得到的向量按行组成3x3的矩阵X\n7.计算注意力权重\n9.Z矩阵蕴含的信息\nZ 矩阵中每一行的含义 Z 矩阵的每一行对应输入序列中的一个 token。这一行向量不再仅仅代表这个 token 本身的语义，而是同时包含了：\n• 该 token 自身的语义信息： 这是由最初的词嵌入提供的。\n• 该 token 与句子中其他 token 的关系： 这是通过自注意力机制计算得到的。注意力权重决定了其他 token 对当前 token 的重要程度。\n• 上下文信息： 通过加权求和，将其他 token 的信息（V 向量）融入到当前 token 的表示中。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/3%E6%9C%88%E4%BB%BD/2025-03-9-ai-language-understanding/","summary":"\u003cp\u003e\u003cstrong\u003eAI大语言模型\u003c/strong\u003e \u003cstrong\u003e(Artificial Intelligence Large Language Model)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e• AI (Artificial Intelligence): \u003cstrong\u003e人工智能\u003c/strong\u003e。这部分表明了AI的本质——不是一个真实的人类，而是通过计算机程序和算法构建出来的智能体。能够执行通常需要人类智能才能完成的任务，比如学习、推理、解决问题、理解语言等等。\u003c/p\u003e","title":"探秘AI大脑：我是如何理解一句话的"},{"content":"当我们与ChatGPT、Siri或其他AI助手对话时，它们似乎能够理解我们的语言并做出适当回应。但AI系统实际上是如何\u0026quot;理解\u0026quot;人类语言的呢？本文将深入探讨现代AI系统处理和理解一句话的完整过程。\n1. 语言理解的基础：从文本到数字 1.1 词嵌入：将词语转化为向量 AI系统无法直接处理文本，它们需要将文本转换为数字形式。这一过程的基础是词嵌入（Word Embeddings）。\n词嵌入技术（如Word2Vec、GloVe或FastText）将每个词映射到高维向量空间中的一个点。这些向量捕捉了词语之间的语义关系，例如：\nvector(\u0026#34;国王\u0026#34;) - vector(\u0026#34;男人\u0026#34;) + vector(\u0026#34;女人\u0026#34;) ≈ vector(\u0026#34;王后\u0026#34;) 在这个向量空间中，语义相似的词会彼此靠近，这使AI系统能够理解词语之间的关系。\n1.2 分词与标记化 在处理一句话之前，AI系统首先需要将句子分解为更小的单位。这一过程称为分词（Tokenization）。\n例如，句子\u0026quot;AI是如何理解一句话的\u0026quot;可能被分解为：[\u0026ldquo;AI\u0026rdquo;, \u0026ldquo;是\u0026rdquo;, \u0026ldquo;如何\u0026rdquo;, \u0026ldquo;理解\u0026rdquo;, \u0026ldquo;一句\u0026rdquo;, \u0026ldquo;话\u0026rdquo;, \u0026ldquo;的\u0026rdquo;]\n不同语言有不同的分词挑战。英语等拉丁语系语言通常以空格和标点为分隔符，而中文等语言则需要更复杂的分词算法。\n2. 深度理解：上下文与语义分析 2.1 从静态表示到动态表示 早期的词嵌入技术为每个词分配一个静态向量，无法处理一词多义的情况。例如，\u0026ldquo;苹果\u0026quot;可以指水果，也可以指科技公司。\n现代AI系统使用上下文化表示（Contextualized Representations），即根据上下文动态生成词语的向量表示：\nvector(\u0026#34;苹果\u0026#34;, context=\u0026#34;我吃了一个苹果\u0026#34;) ≠ vector(\u0026#34;苹果\u0026#34;, context=\u0026#34;苹果公司发布了新iPhone\u0026#34;) 2.2 注意力机制：关注重点 注意力机制（Attention Mechanism）使AI系统能够在处理句子时专注于相关部分。例如，在理解问题\u0026quot;AI如何理解语言？\u0026ldquo;时，系统会关注\u0026quot;AI\u0026rdquo;、\u0026ldquo;理解\u0026quot;和\u0026quot;语言\u0026quot;这些关键词。\nTransformer架构引入的自注意力（Self-Attention）机制使模型能够同时考虑句子中所有词之间的关系，这对于理解长距离依赖和复杂语义至关重要。\n3. 现代语言模型：预训练与微调 3.1 预训练语言模型 现代AI语言理解的核心是预训练语言模型（PLMs），如BERT、GPT、RoBERTa等。这些模型通过在大规模文本上预训练，学习了语言的一般特征和知识。\n预训练任务通常包括：\n掩码语言建模（MLM）：预测被遮蔽的词（如BERT） 自回归语言建模：预测下一个词（如GPT） 语言对比学习：区分真实与随机替换的文本片段 3.2 从理解单句到理解对话 理解单句只是AI语言理解的基础。在实际应用中，AI系统需要理解对话上下文、跨句关系和隐含意图。\n现代对话系统使用对话状态跟踪（Dialogue State Tracking）和上下文建模（Context Modeling）技术来维护对话历史，使系统能够理解与之前交流相关的新输入。\n4. 理解过程的具体步骤：以一句话为例 让我们通过具体例子\u0026quot;今天天气真好，我想去公园散步\u0026rdquo;，来说明AI系统如何逐步理解一句话：\n预处理与分词：\n句子被分解为标记：[\u0026ldquo;今天\u0026rdquo;, \u0026ldquo;天气\u0026rdquo;, \u0026ldquo;真\u0026rdquo;, \u0026ldquo;好\u0026rdquo;, \u0026ldquo;，\u0026rdquo;, \u0026ldquo;我\u0026rdquo;, \u0026ldquo;想\u0026rdquo;, \u0026ldquo;去\u0026rdquo;, \u0026ldquo;公园\u0026rdquo;, \u0026ldquo;散步\u0026rdquo;] 每个标记转换为唯一的ID 向量表示：\n对每个标记生成初始嵌入向量 加入位置编码，告诉模型每个词在句子中的位置 上下文编码：\n通过多层Transformer结构处理这些向量 自注意力机制帮助模型理解\u0026quot;天气好\u0026quot;与\u0026quot;去公园散步\u0026quot;之间的因果关系 语义理解：\n模型识别这是一个陈述句，包含对天气的评价和一个意图 识别\u0026quot;今天\u0026quot;是时间，\u0026ldquo;公园\u0026quot;是地点，\u0026ldquo;散步\u0026quot;是活动 情感分析：\n检测到积极情感（\u0026ldquo;天气真好\u0026rdquo;） 理解这种积极情感与后面的意图之间的联系 5. 挑战与局限性 尽管取得了显著进展，AI语言理解仍面临多项挑战：\n5.1 理解而非模仿 语言模型可能只是在统计模仿语言模式，而非真正理解意义。例如，模型可能生成流畅但无意义的回应。\n5.2 常识推理 AI系统难以掌握人类认为理所当然的常识，如\u0026quot;杯子可以盛水\u0026quot;或\u0026quot;人不能穿墙而过\u0026rdquo;。\n5.3 文化与隐含意义 语言充满文化特定的隐喻、俚语和双关语，这些对AI系统来说特别具有挑战性。\n6. 未来发展方向 6.1 多模态理解 结合视觉、音频和文本信息，使AI系统能像人类一样多角度理解信息。\n6.2 神经符号结合 将神经网络的模式识别能力与符号逻辑的精确推理能力结合，创建更强大的语言理解系统。\n6.3.知识增强型模型 将结构化知识库与语言模型结合，提高系统的常识推理能力和事实准确性。\n结论 现代AI系统通过复杂的神经网络架构、大规模预训练和精细的语义表示，已经能够在一定程度上\u0026quot;理解\u0026quot;人类语言。尽管这种理解与人类的语言理解有本质区别，但其进步已经使人机交流变得比过去任何时候都更加自然和有效。\n随着研究的深入，我们有理由期待AI语言理解能力将继续提升，逐步缩小与人类语言理解的差距。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/3%E6%9C%88%E4%BB%BD/2025-03-9-ai-language-understand/","summary":"\u003cp\u003e当我们与ChatGPT、Siri或其他AI助手对话时，它们似乎能够理解我们的语言并做出适当回应。但AI系统实际上是如何\u0026quot;理解\u0026quot;人类语言的呢？本文将深入探讨现代AI系统处理和理解一句话的完整过程。\u003c/p\u003e","title":"探秘AI：AI是如何理解一句话的"},{"content":"Transformer架构自2017年问世以来彻底改变了自然语言处理领域。从BERT到GPT，从T5到LLaMA，几乎所有当前最先进的语言模型都基于Transformer架构。本文将深入探讨Transformer的核心——注意力机制，包括其数学原理、计算过程和最新的优化方法。\n1. 注意力机制的起源 注意力机制最初源于人类视觉感知的启发。当我们观察复杂场景时，大脑会自动聚焦于相关细节而忽略无关信息。2014年，Bahdanau等人首次将注意力机制引入神经机器翻译任务，使模型能够在生成翻译时动态聚焦于源句子的相关部分。\nTransformer架构中的注意力机制是\u0026quot;自注意力\u0026quot;(Self-Attention)的一种形式，它允许模型考虑序列中所有词之间的关系，而不仅仅是局部上下文。这一机制为模型提供了捕获长距离依赖关系的能力，这是传统RNN和CNN架构的主要局限之一。\n2. 自注意力机制的数学原理 Transformer中的自注意力机制可以表述为对查询向量(Query)、键向量(Key)和值向量(Value)的操作。给定输入序列X，我们首先通过三个不同的变换矩阵W^Q, W^K, W^V计算查询、键和值：\nQ = XW^Q K = XW^K V = XW^V 接下来，通过查询和键的点积计算注意力分数，表示序列中每对词之间的关系强度：\n\\text{注意力分数} = \\frac{QK^T}{\\sqrt{d_k}} 其中d_k是键向量的维度，用于缩放以防止点积结果过大导致softmax梯度消失。\n然后，对注意力分数应用softmax函数，得到注意力权重：\n\\text{注意力权重} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) 最后，将注意力权重与值相乘，得到自注意力的输出：\n\\text{输出} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\times V 3. 多头注意力机制 为了增强模型的表达能力，Transformer使用了多头注意力(Multi-Head Attention)机制。多头注意力并行运行多个自注意力\u0026quot;头\u0026quot;，每个头使用不同的投影矩阵W^Q, W^K, W^V，允许模型同时关注不同的表示子空间：\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O \\\\ \\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) 每个注意力头可以学习关注不同的模式。例如，一些头可能关注语法关系，而其他头可能关注语义相似性或共指关系。这种多角度观察机制显著增强了模型的建模能力。\n4. 注意力机制的计算优化 虽然Transformer的注意力机制非常强大，但其计算复杂度为O(n²)，n为序列长度。这对处理长文本构成了挑战。近年来，研究者提出了多种优化方法：\n4.1 稀疏注意力 稀疏注意力机制如Block Sparse Attention和Longformer只计算部分词对之间的注意力分数，通常基于局部性假设或预定义的稀疏模式。这将复杂度降至O(n log(n))或更低。\n4.2 线性注意力 Performer和Linear Transformer等模型使用核方法近似标准注意力，将复杂度降至O(n)。例如，Performer使用随机特征图将注意力计算重写为：\n\\text{Attention}(Q, K, V) \\approx \\phi(Q)(\\phi(K)^T V) / (\\phi(Q)\\phi(K)^T \\mathbf{1}) 其中φ是随机特征映射，允许我们通过改变乘法顺序将计算复杂度从O(n²)降至O(n)。\n4.3 局部敏感哈希注意力 Reformer使用局部敏感哈希(LSH)将复杂度降至O(n log(n))。LSH将相似的键向量聚类，限制每个查询只与同一哈希桶内的键交互，显著减少计算量。\n5. 结论与展望 注意力机制是Transformer架构的核心创新，为NLP领域带来了革命性突破。随着研究的深入，我们看到了各种注意力变体的出现，如线性注意力、稀疏注意力和局部敏感哈希注意力，它们在保持模型能力的同时大幅提高了计算效率。\n未来的研究方向包括：\n进一步提高注意力机制的计算效率 设计更有效的位置编码方法 探索注意力机制在多模态环境中的应用 开发更强大的注意力可解释性技术 随着计算资源的增长和算法的改进，我们有理由相信，基于注意力机制的模型将继续引领AI领域的发展，并在更广泛的应用场景中发挥作用。\n","permalink":"https://luoziyan100.github.io/myweb/posts/2025/3%E6%9C%88%E4%BB%BD/2025-03-08-transformer-optimization/","summary":"\u003cp\u003eTransformer架构自2017年问世以来彻底改变了自然语言处理领域。从BERT到GPT，从T5到LLaMA，几乎所有当前最先进的语言模型都基于Transformer架构。本文将深入探讨Transformer的核心——注意力机制，包括其数学原理、计算过程和最新的优化方法。\u003c/p\u003e","title":"解构Transformer：注意力机制的深度解析"}]