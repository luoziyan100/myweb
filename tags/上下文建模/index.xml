<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>上下文建模 on yan的AI时代之旅</title><link>https://luoziyan100.github.io/myweb/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1/</link><description>Recent content in 上下文建模 on yan的AI时代之旅</description><generator>Hugo -- 0.149.0</generator><language>zh-cn</language><lastBuildDate>Mon, 01 Sep 2025 10:00:00 +0800</lastBuildDate><atom:link href="https://luoziyan100.github.io/myweb/tags/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1/index.xml" rel="self" type="application/rss+xml"/><item><title>上下文建模：个性化人工智能的未来</title><link>https://luoziyan100.github.io/myweb/posts/2025-09-01-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1/</link><pubDate>Mon, 01 Sep 2025 10:00:00 +0800</pubDate><guid>https://luoziyan100.github.io/myweb/posts/2025-09-01-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1/</guid><description>&lt;p&gt;来源：foundercoho.substack.com/p/context-mode&lt;/p&gt;
&lt;p&gt;本文由DeepVista.ai 首席执行官Jing Conan Wang撰写&lt;/p&gt;
&lt;p&gt;人工智能界的杰出人物 Andrej Karpathy 最近提出了“上下文工程”这一术语。它描述了手动编写提示和数据以指导大型语言模型的复杂艺术。虽然这个概念正在引起广泛关注，但我认为它给我们指明了错误的方向。&lt;/p&gt;
&lt;p&gt;个人人工智能的未来并不在于无休止地设计环境，而是需要彻底转变到我所说的“环境建模”。&lt;/p&gt;
&lt;p&gt;这不仅仅是语义问题——这是临时补丁和真正解决方案之间的区别。&lt;/p&gt;
&lt;h1 id="当前rag系统的局限性"&gt;当前RAG系统的局限性&lt;/h1&gt;
&lt;p&gt;当今的检索增强生成 (RAG) 系统遵循相对简单的范式。它们使用基于规则的系统检索相关信息（通常使用余弦相似度来查找前 k 个最相关的结果），然后将此上下文呈现给大型语言模型进行处理。虽然这种方法在许多场景中已被证明有效，但它也存在很大的局限性。&lt;/p&gt;
&lt;p&gt;不妨把现在的法学硕士（LLM）想象成一群聪明却固执的团队成员。他们擅长处理任何信息，但却总是用自己固有的世界观来解读数据。随着这些模型变得越来越庞大复杂，他们的方法也变得越来越“僵化”，使得开发人员很难影响他们的内部决策过程。&lt;/p&gt;
&lt;h1 id="从工程到建模范式转变"&gt;从工程到建模：范式转变&lt;/h1&gt;
&lt;p&gt;传统的情境工程方法侧重于创建更复杂的规则和算法来管理情境检索。然而，这错失了一个关键的机会。我们不应该仅仅设计更好的规则，而应该转向情境建模——一个能够根据当前情况生成特定情境的动态自适应系统。上下文建模引入了一个与主语言模型 (LLM) 协同工作的个性化模型，充当智能中介，既能理解用户的需求，又能以最佳方式向大型语言模型呈现信息。这种方法认识到，高效的人工智能系统不仅需要强大的模型，还需要智能的上下文管理。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" loading="lazy" src="https://pbs.twimg.com/media/GzvLioVaMAAWVC3?format=jpg&amp;name=medium"&gt;&lt;/p&gt;
&lt;h1 id="从推荐系统中学习"&gt;从推荐系统中学习&lt;/h1&gt;
&lt;p&gt;上下文建模的架构灵感源自成熟的两阶段推荐系统，该系统为当今许多最成功的平台提供支持。这些系统包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;检索阶段：一种快速、高效的系统，处理大量数据，重点是回忆和速度。&lt;/li&gt;
&lt;li&gt;排名阶段：更复杂的系统，注重准确性，从噪声中提取信号以产生最佳结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;RAG 系统从根本上反映了这种架构，但有一个关键区别：它们用大型语言模型取代了传统的排名组件。这种替代使 RAG 系统能够通过自然语言界面解决开放领域问题，超越了传统推荐系统所解决的有限排名问题。&lt;/p&gt;
&lt;p&gt;然而，当前的 RAG 实现在很大程度上忽视了第一阶段基于模型的检索的潜力。尽管业界已经广泛探索了基于规则的检索系统，但智能、自适应上下文建模的机会仍然很大程度上尚未得到开发。&lt;/p&gt;
&lt;h1 id="上下文建模解决方案"&gt;上下文建模解决方案&lt;/h1&gt;
&lt;p&gt;情境建模通过引入专用于动态生成情境的模型来解决这一问题。该模型无需规模庞大或计算成本高昂——它可以是一个专注的、专业的系统，基于相关数据进行训练，能够理解特定领域和用户的需求。&lt;/p&gt;
&lt;p&gt;上下文建模的主要优点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适应性：与基于规则的系统不同，上下文模型可以随着时间的推移学习并适应新的模式和用户行为。&lt;/li&gt;
&lt;li&gt;个性化：这些模型可以根据用户特定的数据进行训练，创造出真正个性化的人工智能体验，了解个人背景和偏好。&lt;/li&gt;
&lt;li&gt;效率：通过使用更小、更专业的模型来生成上下文，系统在提供更智能的上下文管理的同时，还能保持效率。&lt;/li&gt;
&lt;li&gt;开发人员控制：上下文建模为代理开发人员提供了可影响和改进的可训练组件，从而创造了持续学习和优化的机会。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="alt" loading="lazy" src="https://pbs.twimg.com/media/GzvMr-3bEAAyBT5?format=jpg&amp;name=900x900"&gt;&lt;/p&gt;
&lt;h1 id="理想的架构速度与专业化"&gt;理想的架构：速度与专业化&lt;/h1&gt;
&lt;p&gt;为了使上下文建模切实可行，它必须满足一个关键要求：速度。核心 LLM 的延迟已经成为用户体验的一个重大瓶颈。&lt;/p&gt;
&lt;p&gt;目前，主要的解决方法是流式传输响应。然而，流式传输无法缓解第一个令牌的延迟。检索模型的端到端延迟会导致第一个令牌的延迟。任何上下文建模系统都必须非常快，才能避免加剧这种延迟。&lt;/p&gt;
&lt;p&gt;这引出了“思考”模型的概念，这些模型利用自身的内部机制检索上下文并进行推理，最终生成最终答案。从某种意义上说，这些模型执行的是一种特殊形式的上下文建模。然而，它们面临的主要挑战在于这种“思考”过程速度缓慢且计算成本高昂。&lt;/p&gt;
&lt;p&gt;我认为这些单体式“思维”模型只是一个中间步骤。最佳的长期架构将把两个主要任务解耦。它将包含两个协同工作的不同模型，类似于在推荐领域非常成功的两阶段系统：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;快速上下文模型：高度优化、专业化的模型，专门用于以惊人的速度检索和生成最相关的上下文。&lt;/li&gt;
&lt;li&gt;强大的核心模型：接收这种精心策划的上下文并专注于推理、综合和最终响应生成的复杂任务的大型语言模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种双模型方法允许实现专业化，其中每个组件都可以针对其特定任务进行优化，从而毫不妥协地提供速度和智能。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" loading="lazy" src="https://pbs.twimg.com/media/GzvNXh2a4AA1rTG?format=jpg&amp;name=medium"&gt;&lt;/p&gt;
&lt;h1 id="基础设施机遇"&gt;基础设施机遇&lt;/h1&gt;
&lt;p&gt;上下文建模代表了整个 AI 行业普遍的基础设施需求。随着越来越多的组织部署 RAG 系统和 AI 代理，对复杂上下文管理的需求将持续增长。这为构建能够支持各种应用和用例的基础设施提供了机遇。&lt;/p&gt;
&lt;p&gt;上下文建模系统的开发需要机器学习和系统设计方面的专业知识，将推荐系统的经验教训与自然语言处理和生成的独特挑战相结合。&lt;/p&gt;
&lt;h1 id="期待"&gt;期待&lt;/h1&gt;
&lt;p&gt;个性化人工智能的未来并非在于构建越来越庞大的语言模型，而是在于创建能够与这些强大但缺乏灵活性的模型有效协作的智能系统。上下文建模是迈向这一未来的关键一步，它能够赋能既强大又适应性强的人工智能系统。&lt;/p&gt;
&lt;p&gt;随着我们不断进步，成功实施情境建模的组织将在创建真正理解并服务用户的 AI 系统方面拥有显著优势。从情境工程到情境建模的转变不仅仅是一场技术革新，更是对我们如何构建能够大规模适应和个性化的智能系统的根本性重塑。&lt;/p&gt;
&lt;p&gt;问题不在于情境建模是否会成为标准方法，而在于业界能多快认识到它的潜力，并开始构建支持它的基础设施。个性化人工智能的未来取决于我们能否超越静态规则，拥抱动态、智能的情境生成。&lt;/p&gt;
&lt;p&gt;这篇文章把上下文工程推广为上下文建模，特点是分配两个模型：
一个高度专业化的fast模型快速处理上下文
一个强大的核心模型，接收这种精心策划的上下文并专注于推理、综合，最终响应生成复杂任务。&lt;/p&gt;</description><content:encoded><![CDATA[<p>来源：foundercoho.substack.com/p/context-mode</p>
<p>本文由DeepVista.ai 首席执行官Jing Conan Wang撰写</p>
<p>人工智能界的杰出人物 Andrej Karpathy 最近提出了“上下文工程”这一术语。它描述了手动编写提示和数据以指导大型语言模型的复杂艺术。虽然这个概念正在引起广泛关注，但我认为它给我们指明了错误的方向。</p>
<p>个人人工智能的未来并不在于无休止地设计环境，而是需要彻底转变到我所说的“环境建模”。</p>
<p>这不仅仅是语义问题——这是临时补丁和真正解决方案之间的区别。</p>
<h1 id="当前rag系统的局限性">当前RAG系统的局限性</h1>
<p>当今的检索增强生成 (RAG) 系统遵循相对简单的范式。它们使用基于规则的系统检索相关信息（通常使用余弦相似度来查找前 k 个最相关的结果），然后将此上下文呈现给大型语言模型进行处理。虽然这种方法在许多场景中已被证明有效，但它也存在很大的局限性。</p>
<p>不妨把现在的法学硕士（LLM）想象成一群聪明却固执的团队成员。他们擅长处理任何信息，但却总是用自己固有的世界观来解读数据。随着这些模型变得越来越庞大复杂，他们的方法也变得越来越“僵化”，使得开发人员很难影响他们的内部决策过程。</p>
<h1 id="从工程到建模范式转变">从工程到建模：范式转变</h1>
<p>传统的情境工程方法侧重于创建更复杂的规则和算法来管理情境检索。然而，这错失了一个关键的机会。我们不应该仅仅设计更好的规则，而应该转向情境建模——一个能够根据当前情况生成特定情境的动态自适应系统。上下文建模引入了一个与主语言模型 (LLM) 协同工作的个性化模型，充当智能中介，既能理解用户的需求，又能以最佳方式向大型语言模型呈现信息。这种方法认识到，高效的人工智能系统不仅需要强大的模型，还需要智能的上下文管理。</p>
<p><img alt="alt" loading="lazy" src="https://pbs.twimg.com/media/GzvLioVaMAAWVC3?format=jpg&name=medium"></p>
<h1 id="从推荐系统中学习">从推荐系统中学习</h1>
<p>上下文建模的架构灵感源自成熟的两阶段推荐系统，该系统为当今许多最成功的平台提供支持。这些系统包括：</p>
<ul>
<li>检索阶段：一种快速、高效的系统，处理大量数据，重点是回忆和速度。</li>
<li>排名阶段：更复杂的系统，注重准确性，从噪声中提取信号以产生最佳结果。</li>
</ul>
<p>RAG 系统从根本上反映了这种架构，但有一个关键区别：它们用大型语言模型取代了传统的排名组件。这种替代使 RAG 系统能够通过自然语言界面解决开放领域问题，超越了传统推荐系统所解决的有限排名问题。</p>
<p>然而，当前的 RAG 实现在很大程度上忽视了第一阶段基于模型的检索的潜力。尽管业界已经广泛探索了基于规则的检索系统，但智能、自适应上下文建模的机会仍然很大程度上尚未得到开发。</p>
<h1 id="上下文建模解决方案">上下文建模解决方案</h1>
<p>情境建模通过引入专用于动态生成情境的模型来解决这一问题。该模型无需规模庞大或计算成本高昂——它可以是一个专注的、专业的系统，基于相关数据进行训练，能够理解特定领域和用户的需求。</p>
<p>上下文建模的主要优点包括：</p>
<ul>
<li>适应性：与基于规则的系统不同，上下文模型可以随着时间的推移学习并适应新的模式和用户行为。</li>
<li>个性化：这些模型可以根据用户特定的数据进行训练，创造出真正个性化的人工智能体验，了解个人背景和偏好。</li>
<li>效率：通过使用更小、更专业的模型来生成上下文，系统在提供更智能的上下文管理的同时，还能保持效率。</li>
<li>开发人员控制：上下文建模为代理开发人员提供了可影响和改进的可训练组件，从而创造了持续学习和优化的机会。</li>
</ul>
<p><img alt="alt" loading="lazy" src="https://pbs.twimg.com/media/GzvMr-3bEAAyBT5?format=jpg&name=900x900"></p>
<h1 id="理想的架构速度与专业化">理想的架构：速度与专业化</h1>
<p>为了使上下文建模切实可行，它必须满足一个关键要求：速度。核心 LLM 的延迟已经成为用户体验的一个重大瓶颈。</p>
<p>目前，主要的解决方法是流式传输响应。然而，流式传输无法缓解第一个令牌的延迟。检索模型的端到端延迟会导致第一个令牌的延迟。任何上下文建模系统都必须非常快，才能避免加剧这种延迟。</p>
<p>这引出了“思考”模型的概念，这些模型利用自身的内部机制检索上下文并进行推理，最终生成最终答案。从某种意义上说，这些模型执行的是一种特殊形式的上下文建模。然而，它们面临的主要挑战在于这种“思考”过程速度缓慢且计算成本高昂。</p>
<p>我认为这些单体式“思维”模型只是一个中间步骤。最佳的长期架构将把两个主要任务解耦。它将包含两个协同工作的不同模型，类似于在推荐领域非常成功的两阶段系统：</p>
<ul>
<li>快速上下文模型：高度优化、专业化的模型，专门用于以惊人的速度检索和生成最相关的上下文。</li>
<li>强大的核心模型：接收这种精心策划的上下文并专注于推理、综合和最终响应生成的复杂任务的大型语言模型。</li>
</ul>
<p>这种双模型方法允许实现专业化，其中每个组件都可以针对其特定任务进行优化，从而毫不妥协地提供速度和智能。</p>
<p><img alt="alt" loading="lazy" src="https://pbs.twimg.com/media/GzvNXh2a4AA1rTG?format=jpg&name=medium"></p>
<h1 id="基础设施机遇">基础设施机遇</h1>
<p>上下文建模代表了整个 AI 行业普遍的基础设施需求。随着越来越多的组织部署 RAG 系统和 AI 代理，对复杂上下文管理的需求将持续增长。这为构建能够支持各种应用和用例的基础设施提供了机遇。</p>
<p>上下文建模系统的开发需要机器学习和系统设计方面的专业知识，将推荐系统的经验教训与自然语言处理和生成的独特挑战相结合。</p>
<h1 id="期待">期待</h1>
<p>个性化人工智能的未来并非在于构建越来越庞大的语言模型，而是在于创建能够与这些强大但缺乏灵活性的模型有效协作的智能系统。上下文建模是迈向这一未来的关键一步，它能够赋能既强大又适应性强的人工智能系统。</p>
<p>随着我们不断进步，成功实施情境建模的组织将在创建真正理解并服务用户的 AI 系统方面拥有显著优势。从情境工程到情境建模的转变不仅仅是一场技术革新，更是对我们如何构建能够大规模适应和个性化的智能系统的根本性重塑。</p>
<p>问题不在于情境建模是否会成为标准方法，而在于业界能多快认识到它的潜力，并开始构建支持它的基础设施。个性化人工智能的未来取决于我们能否超越静态规则，拥抱动态、智能的情境生成。</p>
<p>这篇文章把上下文工程推广为上下文建模，特点是分配两个模型：
一个高度专业化的fast模型快速处理上下文
一个强大的核心模型，接收这种精心策划的上下文并专注于推理、综合，最终响应生成复杂任务。</p>
<p>它将推荐系统的成功架构应用到了大语言模型中，并提出了一种“双模型”解耦设计：
从“单体式思考”到“双脑协同”：让一个庞大的模型包揽检索、思考、生成所有任务（所谓的“单体式思考模型”）是缓慢且昂贵的。突破点在于将任务解耦：一个“快而专”的上下文模型负责以极高速度处理和生成个性化上下文，另一个“大而强”的核心模型则专注于最终的复杂推理和生成。</p>
<p>以前的上下文是由人主导构建，以后是由一个小模型主导构建。</p>
]]></content:encoded></item></channel></rss>